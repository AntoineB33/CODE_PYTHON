{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the different libraries\n",
    "## Keras libraries are used for the DNN\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes\n",
    "n_games = 50\n",
    "\n",
    "max_mem_size = 4000 #GOTO 10 000 if RAM available\n",
    "\n",
    "K = 4 #Nb of users\n",
    "\n",
    "target_rate_outage = [0.2]*K\n",
    "target_delay_outage = [0.2]*K\n",
    "\n",
    "### Files\n",
    "users_infos_filename = 'MATLAB/ressources/users_infos_K4.mat'\n",
    "los_filename = 'MATLAB/ressources/topologies/topo_3x3/4users/nLoS.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users Infos:\n",
      "[[1.00000000e+00 8.56963734e-01 9.72183169e-01 8.11777547e-01]\n",
      " [1.00000000e+08 5.00000000e+07 5.00000000e+08 1.00000000e+07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.00000000e+00 5.00000000e+00 3.00000000e+00 4.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e-05 1.00000000e-05 1.00000000e-05 1.00000000e-05]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run Schedulers.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DQN's buffer\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, max_size, input_shape, n_actions, discrete = True):\n",
    "        self.mem_size = max_size\n",
    "        self.input_shape = input_shape\n",
    "        self.discrete = discrete\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        \n",
    "        dtype = np.int32 if self.discrete else np.float32\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype = dtype)\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype = np.float32)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = next_state\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "            #if self.discrete:\n",
    "            #Discretize // Find discrete representation of actions\n",
    "            #actions = np.zeros(self.action_memory.shape[1])\n",
    "            #actions[action] = 1.0\n",
    "            #self.action_memory[index] = actions\n",
    "            #else:\n",
    "        self.action_memory[index] = action\n",
    "        self.mem_cntr +=1\n",
    "    \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        next_states = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, next_states, terminal\n",
    "\n",
    "\n",
    "## Function to build the neural network\n",
    "def build_dqn(lr, n_actions, input_dims, fc1_dims, fc2_dims, fc3_dims):\n",
    "    model = Sequential([\n",
    "                Dense(fc1_dims, input_shape=(input_dims, )),\n",
    "                Activation('relu'),\n",
    "                Dense(fc2_dims),\n",
    "                Activation('relu'),\n",
    "                Dense(fc3_dims),\n",
    "                Activation('relu'),\n",
    "                Dense(n_actions)])\n",
    "    model.compile(optimizer = Adam(lr = lr), loss ='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "### Function to transform an integer to a binary action (vector of 1 and 2, 1 = Sub-6 GHz and 2 = mmWave)\n",
    "def to_binary(val):\n",
    "    arr = []\n",
    "    while val:\n",
    "        val, r = divmod(val,2)\n",
    "        arr.append(r)\n",
    "    arr = np.flip(arr)\n",
    "    to_ret = np.append(np.zeros(K - len(arr)), arr).astype(np.int)\n",
    "    to_ret = to_ret + 1\n",
    "    return to_ret.tolist()\n",
    "\n",
    "\n",
    "### Create a ternary action from an integer value (vector of 0, 1 or 2)\n",
    "def to_ternary(val):\n",
    "    arr = []\n",
    "    while val:\n",
    "        val, r = divmod(val,3)\n",
    "        arr.append(r)\n",
    "    arr = np.flip(arr)\n",
    "    to_ret = np.append(np.zeros(K - len(arr)), arr).astype(np.int)\n",
    "    \n",
    "    return to_ret.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size,\n",
    "                    input_dims, epsilon_dec=0.99, epsilon_end=0.01,\n",
    "                    mem_size = max_mem_size, fname_input='models/dqn_model_K8.h5', fname_output='models/dqn_model_K8_pretrained.h5'):\n",
    "        self.action_space = np.arange(n_actions) # Might Have to change with action representation        self.gamma = gamma\n",
    "        self.random_actions_queue = np.arange(n_actions)\n",
    "        np.random.shuffle(self.random_actions_queue)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.batch_size = batch_size\n",
    "        self.model_file_input = fname_input\n",
    "        self.model_file_output = fname_output\n",
    "        \n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions,\n",
    "                                    discrete = True)\n",
    "        self.q_eval = build_dqn(alpha, n_actions, input_dims, 128, 128, 64)\n",
    "        \n",
    "    ## Remember in the DQN's buffer the transitions\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.store_transition(state, action, reward, next_state, done)\n",
    "     \n",
    "    \n",
    "    #### Epsilon greedy method to choose the action\n",
    "    def choose_action(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        rand = np.random.random()\n",
    "        if rand < self.epsilon:\n",
    "            print('random action with epsilon: ', self.epsilon)\n",
    "            action = np.random.choice(self.action_space)   \n",
    "            \n",
    "        else:\n",
    "            print('predicted action with epsilon: ', self.epsilon)\n",
    "            actions = self.q_eval.predict(state)\n",
    "            action = np.argmax(actions[0]) # Maybe actions[0] \n",
    "        return action\n",
    "    \n",
    "    ## Update the DNN\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        if self.memory.mem_cntr > 3*self.batch_size:\n",
    "            self.batch_size = 2*self.batch_size\n",
    "        \n",
    "        state, action, reward, next_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        action_indices = action\n",
    "        \n",
    "        q_eval = self.q_eval.predict(state)\n",
    "        q_next = self.q_eval.predict(next_state)\n",
    "        \n",
    "        q_target = q_eval.copy()\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype = np.int32)\n",
    "        \n",
    "        q_target[batch_index, action_indices] = reward + self.gamma*np.amax(q_next, axis = 1)*done\n",
    "        \n",
    "        \n",
    "        _ = self.q_eval.fit(state, q_target, verbose = 0)\n",
    "        \n",
    "        self.epsilon = self.epsilon*self.epsilon_dec if self.epsilon > self.epsilon_min else self.epsilon_min\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.q_eval.save(self.model_file_output)\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.q_eval = load_model(self.model_file_input)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    ## Initialization\n",
    "    def __init__(self, users_infos_filename, los_filename, target_rate_relab, target_delay_relab):\n",
    "        self.actions_taken = 0\n",
    "        self.infos_filename = users_infos_filename\n",
    "        self.los_filename = los_filename\n",
    "        ## Rate and delay requirements\n",
    "        self.users_rate_requirement = np.ravel(scipy.io.loadmat(self.infos_filename)['users_infos'][1], order = 'F')\n",
    "        self.users_delay_requirement = np.ravel(scipy.io.loadmat(self.infos_filename)['users_infos'][3], order = 'F')\n",
    "        \n",
    "        ### State Space, rate and delay reliability of each user (in the paper, Section 5, I described it as the outage probability and both of the outage or reliab work) (vector of size 2*K) \n",
    "        self.users_infos = np.zeros(2*K)\n",
    "        print(self.users_infos)\n",
    "        self.total_users = int(np.size(self.users_infos)/2)\n",
    "        \n",
    "        ## Rate and delay reliability targets\n",
    "        self.target_rate_relab = 1 - np.array(target_rate_relab)\n",
    "        self.target_delay_relab = 1 - np.array(target_delay_relab)\n",
    "        self.rate_outage_history = []\n",
    "        self.delay_outage_history = []\n",
    "\n",
    "        ## reward weights (Equ. 16 and 17)\n",
    "        self.weights = np.ones(K*2)\n",
    "        \n",
    "        ## Variables used to count the number of time an action has been chosen\n",
    "        self.action_zero = np.zeros(K)\n",
    "        self.action_one = np.zeros(K)\n",
    "        self.action_two = np.zeros(K)\n",
    "        \n",
    "        ## Used to remember the achieved rate/delay per frame and per user\n",
    "        self.rate_perPeriod = []\n",
    "        self.delay_perPeriod = []\n",
    "        \n",
    "    ## Reset all values at the end of an episode\n",
    "    def reset(self):\n",
    "        self.actions_taken = 0\n",
    "        self.users_infos = np.zeros(2*K)\n",
    "        print(self.users_infos)\n",
    "        self.total_users = int(np.size(self.users_infos)/2)  #Can Maybe Comment this line\n",
    "        self.weights = np.ones(K*2)\n",
    "        \n",
    "        self.rate_outage_history = []\n",
    "        self.delay_outage_history = []\n",
    "        \n",
    "        self.action_zero = np.zeros(K)\n",
    "        self.action_one = np.zeros(K)\n",
    "        self.action_two = np.zeros(K)\n",
    "        \n",
    "        return self.users_infos\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Run Matlab code for given action as argument    \n",
    "    def step(self, action, iter_epoch):\n",
    "        self.actions_taken = self.actions_taken + 1\n",
    "        print(self.actions_taken)\n",
    "        ternary_action = to_binary(action) ## Create the action vector\n",
    "        ### Count the number of times an action has been chosen\n",
    "        for idx, val in enumerate(ternary_action):\n",
    "            if val == 0:\n",
    "                self.action_zero[idx] += 1\n",
    "            elif val == 1:\n",
    "                self.action_one[idx] += 1\n",
    "            else:\n",
    "                self.action_two[idx] += 1\n",
    "        print(ternary_action)\n",
    "        \n",
    "        matlab_action = matlab.int32(ternary_action)\n",
    "        \n",
    "        ### Call the matlab optimization code, \n",
    "        ''' Output: \n",
    "        achieved : average achieved rate over the 10 frames per user,\n",
    "        rate_probas : average rate outage probability over the 10 frames per user,\n",
    "        delay_probas : average delay outage probability over the 10 frames per user,\n",
    "        avg_delay : average delay over the 10 frames,\n",
    "        time_opti : optimization time,\n",
    "        total_achieved : achieved rate per frame,\n",
    "        total_delay : achieved delay per frame\n",
    "        '''\n",
    "        \n",
    "        #### CHANGER ICI, CETTE FONCTION UTILISAIT MATLAB MAIS IL FAUT MAINTENANT FAIRE APPEL A LA FONCTION PYTHON\n",
    "        ## On aura qqch comme: \n",
    "        ## achieved, rate_probas, delay_probas, avg_delay, time_opti, total_achieved, total_delay = schedulers(action, self.action_taken, iter_epoch)\n",
    "        achieved, rate_probas, delay_probas, avg_delay, time_opti, total_achieved, total_delay = eng.main_function_K4_N4(matlab_action, los_filename, i+1, self.actions_taken, nargout = 7)\n",
    "        \n",
    "        print('achieved: \\n', achieved)\n",
    "        print('rate_probas: \\n', rate_probas)\n",
    "        print('delay_probas: \\n', delay_probas)\n",
    "        print('average delay: \\n', avg_delay)\n",
    "        print('time opti: \\n', time_opti)\n",
    "        \n",
    "        ach = np.asarray(achieved[0])\n",
    "        ### Compute the reliability (1 - outage proba)\n",
    "        relab_rate = 1 - np.asarray(rate_probas[0])\n",
    "        relab_delay = 1 - np.asarray(delay_probas[0])\n",
    "        delay = np.asarray(avg_delay[0])\n",
    "        \n",
    "        self.rate_outage_history.append(np.asarray(rate_probas[0]))\n",
    "        self.delay_outage_history.append(np.asarray(delay_probas[0]))\n",
    "        \n",
    "        total_achieved = np.asarray(total_achieved)\n",
    "        total_delay = np.asarray(total_delay)\n",
    "        self.rate_perPeriod.append(total_achieved)\n",
    "        self.delay_perPeriod.append(total_delay)\n",
    "        \n",
    "        ### Update the state space (rate and delay reliability values)\n",
    "        self.users_infos[0::2] = relab_rate\n",
    "        self.users_infos[1::2] = relab_delay\n",
    "        \n",
    "        ## Compute the reward and update the weights as described in the paper\n",
    "        reward = - np.sum( self.weights[0::2] * (1 - relab_rate) + self.weights[1::2] * (1 - relab_delay))\n",
    "        self.weights[0::2] = np.maximum(0, self.weights[0::2] + self.target_rate_relab - relab_rate)\n",
    "        self.weights[1::2] = np.maximum(0, self.weights[1::2] + self.target_delay_relab - relab_delay)\n",
    "\n",
    "        print('users_infos updated: \\n', self.users_infos)\n",
    "        print('updated weights:')\n",
    "        print(self.weights[0::2])\n",
    "        print(self.weights[1::2])\n",
    "        \n",
    "        print('Outage means for this period: rate: ', np.mean(np.asarray(rate_probas[0])), ' and delay: ', np.mean(np.asarray(delay_probas[0])))\n",
    "        print('Outage means for this epoch: rate: ', np.mean(self.rate_outage_history), ' and delay: ', np.mean(self.delay_outage_history))\n",
    "    \n",
    "\n",
    "        '''\n",
    "        Condition for the last term of the reward : \n",
    "        boolean, check if the mean of the rate and delay reliability are greater than a threshold (\\gamma* in the paper)\n",
    "        '''\n",
    "        success_rate = np.mean(relab_rate) >= 0.8\n",
    "        success_delay = np.mean(relab_delay) >= 0.8\n",
    "\n",
    "        ### Success condition (Equ. 18)\n",
    "        success = success_rate or success_delay\n",
    "        print(\"Delay: \", success_delay, \" and Rate: \", success_rate)\n",
    "        ## if done = 1, stop the episode. Done = 1 if the success condition is achieved or if we performed 40 learning periods\n",
    "        done = 1 if self.actions_taken >= 20  else success\n",
    "        reward += success*1000\n",
    "        print('Reward: \\n', reward)\n",
    "        \n",
    "        return self.users_infos, reward, done, ach, time_opti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(gamma = 0.1, epsilon = 1, epsilon_dec=0.99, alpha = 0.1, input_dims = 2*K,\n",
    "                  n_actions = (2**K)-1, mem_size = max_mem_size, batch_size=64, epsilon_end = 0.01)\n",
    "\n",
    "\n",
    "## Create the environment\n",
    "env = Environment(users_infos_filename, los_filename, target_rate_outage, target_delay_outage)\n",
    "\n",
    "## Initialize the different arrays to remember the results\n",
    "total_reward_history = []\n",
    "avg_reward_history = []\n",
    "avg_sum_rate = []\n",
    "max_sum_rate = []\n",
    "eps_history = []\n",
    "actions_until_done = []\n",
    "rate_outage = []\n",
    "delay_outage = []\n",
    "\n",
    "time_history_DQN = []\n",
    "time_history_Opti = []\n",
    "\n",
    "action_zero_history = []\n",
    "action_one_history = []\n",
    "action_two_history = []\n",
    "\n",
    "## Start an episode\n",
    "for i in range(n_games):\n",
    "    print('\\n This is a New Episode STARTING: \\n')\n",
    "    \n",
    "    ## Start time to get the execution time\n",
    "    ## Initialize the parameters at the beginning of an episode\n",
    "    start_time = time.time()\n",
    "    time_opti_periode = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    sum_rate = []\n",
    "    observation = env.reset()\n",
    "    \n",
    "    ## For 40 periods or while the condition of Eq. 18 is not reached\n",
    "    while not done:\n",
    "        \n",
    "        ## Choose an action (with the epsilon greedy method)\n",
    "        action = agent.choose_action(observation)\n",
    "        next_observation, reward, done, info, time_opti = env.step(action, i) # TODO CREATE THIS \n",
    "        ## Remember the different information\n",
    "        time_opti_periode += time_opti\n",
    "        total_reward += reward\n",
    "        sum_rate.append(sum(info))\n",
    "        ### Remember the transition in the DQN's buffer\n",
    "        agent.remember(observation, action, reward, next_observation, done)\n",
    "        observation = next_observation\n",
    "        ## Update the DNN\n",
    "        agent.learn()\n",
    "    \n",
    "    ### Remember the different information at the end of an episode, I just use it to create the plots\n",
    "    eps_history.append(agent.epsilon)\n",
    "    total_reward_history.append(total_reward)\n",
    "    avg_sum_rate.append(np.mean(sum_rate))\n",
    "    max_sum_rate.append(np.max(sum_rate))\n",
    "    actions_until_done.append(env.actions_taken)\n",
    "    rate_outage.append(np.mean(env.rate_outage_history, axis = 0))\n",
    "    delay_outage.append(np.mean(env.delay_outage_history, axis = 0))\n",
    "    \n",
    "    ## Compute and remember the total execution time, the learning time and the optimization time\n",
    "    period_time_DQN = time.time() - start_time - time_opti_periode\n",
    "    time_history_DQN.append(period_time_DQN)\n",
    "    time_history_Opti.append(time_opti_periode)\n",
    "    \n",
    "    ## Remember the chosen actions\n",
    "    action_zero_history.append(env.action_zero)\n",
    "    action_one_history.append(env.action_one)\n",
    "    action_two_history.append(env.action_two)\n",
    "\n",
    "        \n",
    "    ## Compute and remember the average reward \n",
    "    avg_reward = np.mean(total_reward_history[max(0, i - 20):(i+1)])\n",
    "    avg_reward_history.append(avg_reward)\n",
    "    print('episode ', i, 'score %.2f' % total_reward, 'average score %.2f' % avg_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
